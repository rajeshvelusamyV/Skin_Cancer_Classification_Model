{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information for Boothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for later\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Background\n",
    "\n",
    "- AIMSS invited us to the symposium, so we decided to make a project for it\n",
    "- Gino, one of our execs, found the skin cancer dataset on kaggle, and we eventually decided to pair it with the web app\n",
    "- We decided to use methods we'd specifically learned in the club, or in undergrad classes\n",
    "\n",
    "### Dataset\n",
    "https://www.kaggle.com/fanconic/skin-cancer-malignant-vs-benign\n",
    "\n",
    "Skin Cancer: Malignant vs Benign\n",
    "\n",
    "This dataset contains a balanced dataset of images of benign skin moles and malignant skin moles. The data consists of two folders with each 1800 pictures (224x244) of the two types of moles. All the rights of the Data are bound to the ISIC-Archive rights (https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Summary\n",
    "Build a webapp that allows a user compete against an AI model at classifying images of skin moles that are potentially benign or malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology Used\n",
    "- The model itself was built using the fastai library, which is a library built over top of pytorch to make building practical deep learning models as easy as possible. Pytorch was also used directly to create activation heat maps\n",
    "- The webapp was built using flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Used: Resnet-50 Variant\n",
    "Resnet 50 is a Neural Network Architecture that is well known for winning the ImageNet Classification challenge in 2015.  It is particularily good for users with less training infrastructure, because it has a high ratio of effectiveness to required training time. The main features of our variant are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Resnet-50](https://miro.medium.com/max/2462/1*zbDxCB-0QDAc4oUGVtg3xw.png)\n",
    "\n",
    "Figure from \"Illustrated: 10 CNN Architectures, by Raimi Karim\" on towardsdatascience.com: https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Features\n",
    "#### Convolutional Layers:\n",
    "A convolutional neural net layer maps different convolutinal kernels over top of its input.  The convolutional kernels act as filters, that highlight specific parts of the input image.  This mechanism takes advantage of the spacial invariance in certain kinds of data, like images.  Lower down in the network, they only detect simple representations, like edges.  At higher layers in a neural network, they are able to combine the simpler representations from lower down in the network to detect things like shapes or textures.\n",
    "\n",
    "![Convolutional NN](https://in.mathworks.com/help/deeplearning/ug/deep_learning_architecture600pixels.png)\n",
    "Figure from \"Learn About Convolutional Neural Networks\" from Mathworks: https://in.mathworks.com/help/deeplearning/ug/introduction-to-convolutional-neural-networks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections\n",
    "Skip connections are basically connections in the network that carry the input signal around a layer, or set of layers. This addition allows all layers to train right away, because they don't have to wait for the layers below them to become coherent. This allows the training of much deeper networks than is usuallly possible, at a much faster rate.\n",
    "![Skip Connection](https://kharshit.github.io/img/resnet_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Layers\n",
    "Theres still argument about the exact effects of batch normalization, but it improves the training stability of the network and reduces overfitting.  It does this by first normalizing the inputs by subtracting the batch mean and then dividing by the batch standard deviation. In the short term, this causes a mismatch with the next layer that optimizer may try to rectify by changing a bunch of weights.  Batch norm stops this by adding in two trainable parameters, for scaling and shifting, that the network can instead change directly to denormalize the output for the next layer.\n",
    "\n",
    "![batch-norm algorithm](https://miro.medium.com/max/526/1*Hiq-rLFGDpESpr8QNsJ1jg.png)\n",
    "\n",
    "Figure from the original batch norm paper: https://arxiv.org/pdf/1502.03167v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layers\n",
    "Dropout provides regularization(model generalization) by randomly removing hidden units during the training process, at some constant percentage. This seems to prevent overfitting by forcing the network connections to be more general, as they need to keep operating even with neurons missing.  It's equivalent to creating an ensemble from a number of similar neural networks to create a single more general one.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1357/1*iWQzxhVlvadk6VAJjsgXgg.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Figure from original dropout paper: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methods\n",
    "The specifics of techniques used during the training process are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Make variations of the starting data items to artificially increase training set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "Start with a network trained on a dataset that is much larger, for a similar task, if possible.  If the data is similar enough, a bunch of the internal representations can be carried over, and this will increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW optimiser\n",
    "Currently the b optimser that uses momentum and stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Cycle Training\n",
    "Increase the learning rate to a peak, then decrease it back to a minimum over the course of each epoch. Do the opposite for momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-16 Precision Arithmetic\n",
    "If you use lower precision for training, you can train way faster, and the noise from the reduced precision will actually increase accuracy, up to a point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-cv3",
   "language": "python",
   "name": "fastai-cv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
